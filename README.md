# ğŸŒ¸ Task 6: K-Nearest Neighbors (KNN) Classification

### ğŸ¯ Objective
To understand and implement the **K-Nearest Neighbors (KNN)** algorithm for a multi-class classification problem using the **Iris Dataset**.

---

### ğŸ“˜ Concepts Covered
- K-Nearest Neighbors (KNN) Algorithm  
- Feature Normalization (Standardization)  
- Choosing the Optimal K  
- Confusion Matrix & Accuracy  
- Decision Boundary Visualization  

---

### ğŸ§© Tools Used
- Python  
- Scikit-learn  
- Pandas  
- Matplotlib  
- Seaborn  
- NumPy  

---

### âš™ï¸ Steps Performed
1. Loaded the Iris dataset  
2. Normalized features using `StandardScaler`  
3. Split the data into training & testing sets  
4. Trained a KNN classifier for different K values  
5. Evaluated model using accuracy and confusion matrix  
6. Visualized accuracy vs K graph  
7. Plotted decision boundaries using 2 features  

---

### ğŸ“ˆ Results
| K | Accuracy |
|---|-----------|
| 1 | 0.90 |
| 3 | 0.93 |
| 5 | 0.96 |
| **Best K** | **Varies (usually 5â€“7)** |

---

### ğŸ§  Learnings
- KNN is a **lazy learning algorithm** â€” it memorizes training data.  
- Choosing a good `K` prevents overfitting (small K) or underfitting (large K).  
- Normalization is crucial for accurate distance measurement.  
- KNN works well for small datasets like Iris.

---

### ğŸš€ How to Run
1. Open the notebook in **Google Colab**  
2. Run all cells sequentially  
3. Observe accuracy results and visualizations  
4. Save the notebook & upload it to GitHub  

---

### ğŸ‘¨â€ğŸ’» Author
**Palli Tulasiram**  
AI & ML Internship â€“ Task 6  
